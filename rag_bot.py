from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader, JSONLoader
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
import os
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Charger les variables d'environnement
load_dotenv()

def load_vectorstore():
    """Charger et indexer les documents"""
    try:
        logger.info("üìÇ Chargement des documents...")
        documents = []
        
        # Charger les PDF
        pdf_files = ["data/HSE_AAOURIDA.pdf", "data/STEULER-HSE-Management.pdf"]
        for pdf_file in pdf_files:
            if os.path.exists(pdf_file):
                logger.info(f"üìÑ Chargement de {pdf_file}...")
                loader = PyPDFLoader(pdf_file)
                docs = loader.load()
                documents.extend(docs)
                logger.info(f"‚úÖ PDF charg√©: {pdf_file} ({len(docs)} pages)")
            else:
                logger.warning(f"‚ö†Ô∏è Fichier non trouv√©: {pdf_file}")
        
        # Charger le JSON avec une approche plus simple
        json_file = "data/mining_safety_database.json"
        if os.path.exists(json_file):
            try:
                import json
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Convertir chaque √©l√©ment JSON en document
                from langchain.schema import Document
                for item in data:
                    if isinstance(item, dict):
                        content = json.dumps(item, ensure_ascii=False)
                        doc = Document(
                            page_content=content,
                            metadata={"source": json_file, "type": "json"}
                        )
                        documents.append(doc)
                
                logger.info(f"‚úÖ JSON charg√©: {json_file} ({len(data)} √©l√©ments)")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur chargement JSON: {e}")
        else:
            logger.warning(f"‚ö†Ô∏è Fichier JSON non trouv√©: {json_file}")

        if not documents:
            raise Exception("Aucun document trouv√© √† indexer")

        logger.info(f"üìä {len(documents)} documents charg√©s")

        # Diviser les documents
        logger.info("‚úÇÔ∏è Division des documents en chunks...")
        splitter = CharacterTextSplitter(
            chunk_size=1000, 
            chunk_overlap=200,
            separator="\n"
        )
        docs = splitter.split_documents(documents)
        logger.info(f"‚úÖ {len(docs)} chunks cr√©√©s")

        # Cr√©er la base vectorielle
        logger.info("üî¢ Cr√©ation des embeddings...")
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        logger.info("üíæ Cr√©ation de la base vectorielle...")
        vectordb = Chroma.from_documents(
            documents=docs, 
            embedding=embeddings, 
            persist_directory="db"
        )
        vectordb.persist()
        logger.info("‚úÖ Base vectorielle cr√©√©e et sauvegard√©e")
        return vectordb
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du chargement: {e}")
        raise

def get_chain():
    """Cr√©er la cha√Æne RAG"""
    try:
        logger.info("üîó Cr√©ation de la cha√Æne RAG...")
        
        # V√©rifier la cl√© API
        api_key = os.getenv("GROQ_API_KEY")
        if not api_key:
            raise Exception("GROQ_API_KEY manquante")
        logger.info("‚úÖ Cl√© API v√©rifi√©e")
        
        # Charger ou cr√©er la base vectorielle
        logger.info("üóÉÔ∏è Chargement de la base vectorielle...")
        try:
            embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            
            # V√©rifier si la base existe
            if os.path.exists("db") and os.listdir("db"):
                vectordb = Chroma(persist_directory="db", embedding_function=embeddings)
                logger.info("üìä Base vectorielle existante charg√©e")
            else:
                logger.info("Cr√©ation d'une nouvelle base vectorielle...")
                vectordb = load_vectorstore()
                
        except Exception as e:
            logger.warning(f"Erreur de chargement de la base existante: {e}")
            logger.info("Cr√©ation d'une nouvelle base vectorielle...")
            vectordb = load_vectorstore()
        
        logger.info("üîç Cr√©ation du retriever...")
        retriever = vectordb.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3}
        )
        
        # Cr√©er le LLM
        logger.info("ü§ñ Cr√©ation du mod√®le de langage...")
        llm = ChatOpenAI(
            openai_api_key=api_key,
            base_url="https://api.groq.com/openai/v1",
            model="llama-3.1-8b-instant",
            temperature=0.1,
            max_tokens=1000,
            timeout=60
        )
        logger.info("‚úÖ Mod√®le de langage cr√©√©")
        
        # Template de prompt
        logger.info("üìù Cr√©ation du template de prompt...")
        prompt_template = """Tu es un assistant sp√©cialis√© en s√©curit√© HSE pour le domaine minier OCP.

Tu dois ABSOLUMENT utiliser les informations du contexte fourni pour r√©pondre aux questions.
Ne dis JAMAIS "je ne dispose pas d'informations" ou "je n'ai pas assez d'informations".

Utilise TOUJOURS le contexte disponible pour construire une r√©ponse pertinente, m√™me si l'information n'est pas compl√®te.
Si le contexte ne mentionne pas exactement ce qui est demand√©, extrais les informations les plus proches et adapte ta r√©ponse.

CONTEXTE DISPONIBLE:
{context}

QUESTION: {question}

R√âPONSE (en te basant sur le contexte ci-dessus):"""
        
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # Cr√©er la cha√Æne
        logger.info("‚õìÔ∏è Assemblage de la cha√Æne finale...")
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": PROMPT}
        )
        
        logger.info("‚úÖ Cha√Æne RAG cr√©√©e avec succ√®s")
        return qa_chain
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la cr√©ation de la cha√Æne: {e}")
        import traceback
        logger.error(f"D√©tails de l'erreur: {traceback.format_exc()}")
        raise

# Classe RAGSystem pour compatibilit√©
class RAGSystem:
    def __init__(self):
        self.chain = None
        self.logger = logging.getLogger(f"{__name__}.RAGSystem")
        
    def initialize(self):
        try:
            self.logger.info("üöÄ D√©but de l'initialisation RAGSystem...")
            self.chain = get_chain()
            self.logger.info("‚úÖ RAGSystem initialis√© avec succ√®s")
            return True
        except Exception as e:
            self.logger.error(f"‚ùå Erreur d'initialisation RAGSystem: {e}")
            import traceback
            self.logger.error(f"Traceback complet: {traceback.format_exc()}")
            return False
    
    def query(self, question):
        if not self.chain:
            return {"error": "Syst√®me non initialis√©"}
        
        try:
            result = self.chain({"query": question})
            return {
                "answer": result["result"],
                "sources": [doc.metadata.get("source", "Inconnu") 
                           for doc in result.get("source_documents", [])]
            }
        except Exception as e:
            self.logger.error(f"Erreur lors de la requ√™te: {e}")
            return {"error": str(e)}

# Fonction de diagnostic de la base vectorielle
def diagnose_vectorstore():
    """Diagnostiquer le contenu de la base vectorielle"""
    try:
        import os
        from langchain.embeddings import HuggingFaceEmbeddings
        from langchain.vectorstores import Chroma
        
        if os.path.exists("db"):
            embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            
            vectordb = Chroma(persist_directory="db", embedding_function=embeddings)
            
            # Test de recherche
            test_results = vectordb.similarity_search("r√©glementation HSE", k=5)
            
            print("=== DIAGNOSTIC VECTORSTORE ===")
            print(f"Nombre de documents trouv√©s: {len(test_results)}")
            
            for i, doc in enumerate(test_results):
                print(f"\n--- Document {i+1} ---")
                print(f"Source: {doc.metadata.get('source', 'Inconnue')}")
                print(f"Contenu (100 premiers caract√®res): {doc.page_content[:100]}...")
                
            return test_results
        else:
            print("‚ùå Aucune base vectorielle trouv√©e")
            return []
            
    except Exception as e:
        print(f"‚ùå Erreur diagnostic: {e}")
        return []

# Fonction de test simple
def test_system():
    """Fonction pour tester le syst√®me sans Streamlit"""
    try:
        rag = RAGSystem()
        if rag.initialize():
            print("‚úÖ Syst√®me initialis√© avec succ√®s")
            
            # Test simple
            result = rag.query("Quels sont les √©quipements de protection ?")
            print(f"R√©ponse: {result}")
            return True
        else:
            print("‚ùå √âchec de l'initialisation")
            return False
    except Exception as e:
        print(f"‚ùå Erreur de test: {e}")
        return False

# Fonction de test avec diagnostic
def test_system_with_diagnosis():
    """Test avec diagnostic d√©taill√©"""
    print("üîç DIAGNOSTIC DU SYST√àME")
    
    # 1. V√©rifier les fichiers
    files_to_check = [
        "data/HSE_AAOURIDA.pdf",
        "data/STEULER-HSE-Management.pdf", 
        "data/mining_safety_database.json"
    ]
    
    for file_path in files_to_check:
        if os.path.exists(file_path):
            size = os.path.getsize(file_path)
            print(f"‚úÖ {file_path} ({size} bytes)")
        else:
            print(f"‚ùå {file_path} non trouv√©")
    
    # 2. Diagnostiquer la base vectorielle
    diagnose_vectorstore()
    
    # 3. Tester le syst√®me
    try:
        rag = RAGSystem()
        if rag.initialize():
            print("\n‚úÖ Syst√®me initialis√© avec succ√®s")
            
            # Tests de questions
            test_questions = [
                "r√©glementation HSE",
                "√©quipements de protection",
                "s√©curit√© mini√®re",
                "proc√©dures"
            ]
            
            for question in test_questions:
                print(f"\nüîç Test: {question}")
                result = rag.query(question)
                if "error" not in result:
                    print(f"‚úÖ R√©ponse trouv√©e ({len(result['answer'])} caract√®res)")
                    print(f"üìö Sources: {result.get('sources', [])}")
                else:
                    print(f"‚ùå Erreur: {result['error']}")
            
            return True
        else:
            print("‚ùå √âchec de l'initialisation")
            return False
    except Exception as e:
        print(f"‚ùå Erreur de test: {e}")
        return False

if __name__ == "__main__":
    test_system_with_diagnosis()